{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# brew install libmagic\n",
    "# https://python.langchain.com/v0.2/docs/how_to/document_loader_directory/"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_community.document_loaders import DirectoryLoader\n",
    "loader = DirectoryLoader('../documents', glob=\"sec_filing_tables.txt\")\n",
    "docs = loader.load()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "len(docs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "docs[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_text_splitters import RecursiveCharacterTextSplitter\n",
    "\n",
    "# Load example document\n",
    "with open(\"../documents/sec_filing_tables.txt\") as f:\n",
    "    state_of_the_union = f.read()\n",
    "\n",
    "text_splitter = RecursiveCharacterTextSplitter(\n",
    "    # Set a really small chunk size, just to show.\n",
    "    chunk_size=500,\n",
    "    chunk_overlap=100,\n",
    "    length_function=len,\n",
    "    is_separator_regex=[\n",
    "        \"\\n\\n\",\n",
    "        \"\\n\",\n",
    "        \" \",\n",
    "        \".\",\n",
    "        \",\",\n",
    "        \"\\u200b\",  # Zero-width space\n",
    "        \"\\uff0c\",  # Fullwidth comma\n",
    "        \"\\u3001\",  # Ideographic comma\n",
    "        \"\\uff0e\",  # Fullwidth full stop\n",
    "        \"\\u3002\",  # Ideographic full stop\n",
    "        \"\",\n",
    "    ]\n",
    ")\n",
    "texts = text_splitter.create_documents([state_of_the_union])\n",
    "print(texts[0])\n",
    "print(texts[1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# pip install langchain faiss-cpu tiktoken openai"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain.document_loaders import TextLoader\n",
    "from langchain.text_splitter import CharacterTextSplitter\n",
    "from langchain.embeddings import OpenAIEmbeddings\n",
    "from langchain.vectorstores import FAISS\n",
    "from langchain.chains import ConversationalRetrievalChain\n",
    "from langchain.llms import OpenAI\n",
    "from langchain.memory import ConversationBufferMemory\n",
    "import tiktoken\n",
    "\n",
    "# Step 1: Load the text file\n",
    "text_loader = TextLoader('../documents/sec_filing_combined.txt')\n",
    "documents = text_loader.load()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Find tokens to calculate number of tokens in the text to find cost\n",
    "text_content = documents[0].page_content\n",
    "# Step 2: Tokenize the text\n",
    "# Select the appropriate tokenizer for the OpenAI model you'll be using\n",
    "tokenizer = tiktoken.get_encoding('cl100k_base')  # Adjust encoding for your specific model if necessary\n",
    "tokens = tokenizer.encode(text_content)\n",
    "\n",
    "# Calculate the number of tokens\n",
    "num_tokens = len(tokens)\n",
    "print(f\"Total number of tokens in the text file: {num_tokens}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Step 3: Estimate cost for embeddings\n",
    "# Assume you're using OpenAI's embedding model with cost estimates\n",
    "# Example costs: $0.0004 per 1K tokens for text-embedding-ada-002 (as of August 2024)\n",
    "cost_per_1k_tokens = 0.0004\n",
    "cost = (num_tokens / 1000) * cost_per_1k_tokens\n",
    "print(f\"Estimated cost for generating embeddings: ${cost:.4f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Step 2: Split the text into smaller chunks\n",
    "text_splitter = RecursiveCharacterTextSplitter(chunk_size=1000, chunk_overlap=200)\n",
    "split_documents = text_splitter.split_documents(documents)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Step 3: Generate embeddings for the text chunks\n",
    "embeddings = OpenAIEmbeddings()\n",
    "doc_embeddings = embeddings.embed_documents([doc.page_content for doc in split_documents])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "type(doc_embeddings)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "doc_embeddings[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Step 4: Store the embeddings in a local FAISS vector store\n",
    "vectorstore = FAISS.from_documents(split_documents, embeddings)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "retriever = vectorstore.as_retriever(search_type=\"similarity\", search_kwargs={\"k\": 4})\n",
    "\n",
    "retrieved_docs = retriever.invoke(\"Tell me about Microsoft Cloud gross margin?\")\n",
    "\n",
    "len(retrieved_docs)\n",
    "retrieved_docs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain.chains import ConversationalRetrievalChain\n",
    "from langchain.llms import OpenAI\n",
    "\n",
    "# Assuming you've already created the vector store and retriever as in previous steps\n",
    "retriever = vectorstore.as_retriever(search_type=\"similarity\", search_kwargs={\"k\": 4})\n",
    "\n",
    "# Set up the conversational retrieval chain without memory\n",
    "llm = OpenAI()\n",
    "qa_chain = ConversationalRetrievalChain.from_llm(llm, retriever, memory=None)\n",
    "\n",
    "# Query the stored embeddings for similar documents\n",
    "query = \"Tell me about Microsoft Cloud gross margin?\"\n",
    "response = qa_chain.run({\"question\": query, \"chat_history\": []})\n",
    "\n",
    "print(response)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### If you want more control over the process and avoid using chains that might be missing, you can manually retrieve the documents and then pass them to the LLM for answering:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain.llms import OpenAI\n",
    "from langchain.prompts import PromptTemplate\n",
    "from langchain.chains import LLMChain\n",
    "\n",
    "# Assuming you've already created the vector store and retriever as in previous steps\n",
    "retriever = vectorstore.as_retriever(search_type=\"similarity\", search_kwargs={\"k\": 10})\n",
    "\n",
    "# Define a prompt template for LLM processing\n",
    "prompt_template = \"\"\"\n",
    "You are a finance expert known to assess 10 K documents and provide data as an input to Sankey charts. \n",
    "Based on the following documents, provide factually correct answers from document. Be concise and precise.\n",
    "{documents}\n",
    "\n",
    "Question: {question}\n",
    "Answer:\n",
    "\"\"\"\n",
    "\n",
    "prompt = PromptTemplate(template=prompt_template, input_variables=[\"documents\", \"question\"])\n",
    "\n",
    "\n",
    "# Set up the LLM and chain\n",
    "llm = OpenAI()\n",
    "qa_chain = LLMChain(llm=llm, prompt=prompt)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "| Business Segment | Revenue (in millions) |\n",
      "|------------------|-----------------------|\n",
      "| Server products and cloud services | $97,726 |\n",
      "| Office products and cloud services | $54,875 |\n",
      "| Windows | $23,244 |\n",
      "| Gaming | $21,503 |\n",
      "| LinkedIn | $16,372 |\n",
      "| Search and news advertising | $12,576 |\n",
      "| Enterprise and partner services | $7,594 |\n",
      "| Dynamics products and cloud services | $6,481 |\n",
      "| Devices | $4,706 |\n",
      "| Other | $45 |\n",
      "| Total | $245,122 |\n"
     ]
    }
   ],
   "source": [
    "# Retrieve documents\n",
    "query = \"in table format provide the revenue generated from each business like azure, linkedin etc?\"\n",
    "retrieved_docs = retriever.invoke(query)\n",
    "\n",
    "# Combine the retrieved document content\n",
    "combined_docs_content = \"\\n\".join([doc.page_content for doc in retrieved_docs])\n",
    "\n",
    "# Run the chain with the retrieved documents and query\n",
    "response = qa_chain.run({\"documents\": combined_docs_content, \"question\": query})\n",
    "\n",
    "print(response)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Query the stored embeddings for similar documents\n",
    "query = \"Tell me about Microsoft Cloud gross margin?\"\n",
    "response = qa_chain.run({\"question\": query, \"chat_history\": []})\n",
    "\n",
    "print(response)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Step 5: Set up a conversational retrieval chain\n",
    "memory = ConversationBufferMemory(memory_key=\"chat_history\", return_messages=True)\n",
    "retriever = vectorstore.as_retriever(search_kwargs={\"k\": 3})\n",
    "llm = OpenAI()\n",
    "qa_chain = ConversationalRetrievalChain.from_llm(llm, retriever, memory=memory)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Step 6: Query the stored embeddings\n",
    "query = \"Tell me about Microsoft Cloud gross margin\"\n",
    "response = qa_chain.run({\"question\": query, \"chat_history\": []})\n",
    "\n",
    "print(response)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Chat loop (optional)\n",
    "while True:\n",
    "    query = input(\"Ask a question: \")\n",
    "    response = qa_chain.run({\"question\": query, \"chat_history\": []})\n",
    "    print(response)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
